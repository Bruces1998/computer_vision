{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "inputs = np.random.random((timestamps, input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "    \n",
    "final_output_sequence = np.concatenate(successive_outputs, axis = 0)\n",
    "output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randn(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr(:,:,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in arr:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.random.randn(3,4)\n",
    "a2 = np.random.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((a1,a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((a_prev_tmp, xt_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: lstm_forward\n",
    "\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_y, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:,:,t]\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt,a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the next cell state (≈1 line)\n",
    "        c[:,:,t]  = c_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y[:,:,t] = yt\n",
    "        # Append the cache into caches (≈1 line)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = {'a' : 1, 'b' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se[dino] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ndarray((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0] = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(arr, 1,3, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer as lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(['spam', 'spam', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((3,))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "a_prev = np.zeros((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prev.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(char_to_ix[i] for i in char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b =[ i for i in a ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rnn_cell(Tx, x0, a0):\n",
    "# Tx = 100\n",
    "# x0 = 32\n",
    "# a0 = 64\n",
    "    inputs = np.random.random((Tx, x0))\n",
    "    state_t = np.zeros((64,))\n",
    "    W = np.random.random((a0, x0))\n",
    "    U = np.random.random((a0, a0))\n",
    "    b = np.random.random((a0,))\n",
    "\n",
    "    successive_outputs = []\n",
    "\n",
    "    for input_t in inputs:\n",
    "\n",
    "        output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "        successive_outputs.append(output_t)\n",
    "\n",
    "        state_t = output_t\n",
    "\n",
    "    final = np.concatenate(successive_outputs, axis = 0)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rnn_cell(100, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences = True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking the multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CosineSimilarity\n",
    "np.random.seed(1)\n",
    "u = np.random.randn(50)\n",
    "v = np.random.randn(50)\n",
    "\n",
    "def norm(u):\n",
    "    sum = 0\n",
    "    for i in u:\n",
    "        sum = sum + i**2\n",
    "        \n",
    "    return i**0.5\n",
    "\n",
    "def CosSim(u, v):\n",
    "    nor_u = norm(u)\n",
    "    nor_v = norm(v)\n",
    "    return (np.dot(u, v)/(nor_u*nor_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(u):\n",
    "    sum = 0\n",
    "    for i in u:\n",
    "        sum = sum + i**2\n",
    "        \n",
    "    return i**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosSim(u, v):\n",
    "    nor_u = norm(u)\n",
    "    nor_v = norm(v)\n",
    "    return (np.dot(u, v)/(nor_u*nor_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CosSim(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found {} unique tokens.'.format(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lb.fit_transform(samples[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, data2 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = \"aclImdb/aclImdb/\"\n",
    "train_dir = os.path.join(imdb_dir,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "val_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of Data tensor:', data.shape)\n",
    "print('Shape of Label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Till here we have downloaded raw imdb dataset and preprocessed it to convert it into form of tokens where each word is now represented by a token number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples:training_samples + val_samples]\n",
    "y_val = labels[training_samples:training_samples + val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = 'glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(glove_dir, 'glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "file.close()\n",
    "print('found {} word vectors'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Till here we have downloaded the glove embedding and loaded it in the dictionary in the form dic[word] = feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo',label = 'Traing acc')\n",
    "plt.plot(epochs,  val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo',label = 'Traing acc')\n",
    "plt.plot(epochs,  val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "                \n",
    "            else:\n",
    "                labels.append(1)\n",
    "                \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "    \n",
    "    \n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilayer rnn\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('[INFO] LOADING DATA...... ')\n",
    "\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "num_words=max_features)\n",
    "\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print(input_train.shape, input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo',label = 'Traing acc')\n",
    "plt.plot(epochs,  val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 500\n",
    "print(\"[INFO] Loading Data.....\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPool1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss = \"binary_crossentropy\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo',label = 'Traing acc')\n",
    "plt.plot(epochs,  val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \n",
    "    Waa = parameters['Waa']\n",
    "    Wax = parameters['Wax']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Wax, xt) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    cache = (a_next, yt_pred, xt, parameters)\n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba'] = np.random.randn(5,1)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
    "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
    "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x)/np.sum(np.exp(x), axis=0))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+(np.e)**-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev,c_prev, parameters):\n",
    "    Wf =  parameters['Wf']\n",
    "    Wc =  parameters['Wc']\n",
    "    Wu =  parameters['Wu']\n",
    "    Wo = parameters['Wo']\n",
    "    bf = parameters['bf']\n",
    "    bc = parameters['bc']\n",
    "    bu = parameters['bu']\n",
    "    bo =parameters['bo']\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_x,m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    concat = np.concatenate((a_prev, xt))\n",
    "    \n",
    "    ft =sigmoid(np.dot(Wf,concat) + bf)\n",
    "    ut =sigmoid(np.dot(Wu, concat)+ bu)\n",
    "    ot = sigmoid(np.dot(Wo, concat)+bo)\n",
    "    cct = np.tanh(np.dot(Wc,concat)+bc)\n",
    "    c_next = ut*cct + ft*c_prev\n",
    "    \n",
    "    a_next = ot* np.tanh(c_next)\n",
    "    \n",
    "    \n",
    "    yt_pred = sigmoid(np.dot(Wy, a_next)+ by)\n",
    "    \n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, ut, cct, ot, xt, parameters)\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "c_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wu'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bu'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \", c_next_tmp.shape)\n",
    "print(\"c_next[2] = \\n\", c_next_tmp[2])\n",
    "print(\"c_next.shape = \", c_next_tmp.shape)\n",
    "print(\"yt[1] =\", yt_tmp[1])\n",
    "print(\"yt.shape = \", yt_tmp.shape)\n",
    "print(\"cache[1][3] =\\n\", cache_tmp[1][3])\n",
    "print(\"len(cache) = \", len(cache_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    caches = []\n",
    "    \n",
    "    Wy = parameters['Wy']\n",
    "    \n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        xt = x[:,:,t]\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt,a_next, c_next, parameters)\n",
    "        a[:,:,t] = c_next\n",
    "        y[:,:,t] = yt\n",
    "        \n",
    "        caches.append(cache)\n",
    "        \n",
    "        \n",
    "        \n",
    "    caches = (caches,x)\n",
    "    \n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wu = np.random.randn(5, 5+3)\n",
    "bu = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wu\": Wu, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bu\": bu, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('dino.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('shakespeare.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n",
    "    \n",
    "    Arguments:\n",
    "    text -- string, corpus of Shakespearian poem\n",
    "    Tx -- sequence length, number of time-steps (or characters) in one training example\n",
    "    stride -- how much the window shifts itself while scanning\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines)\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    print('number of training examples:', len(X))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) into arrays to be given to a recurrent neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns:\n",
    "    x -- array of shape (m, Tx, len(chars))\n",
    "    y -- array of shape (m, len(chars))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x))\n",
    "    y = np.zeros((m, n_x))\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = io.open('shakespeare.txt',encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 40\n",
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c,i) for i,c in enumerate(chars))\n",
    "indices_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Training Set\")\n",
    "X, Y = build_data(text, Tx, stride=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = vectorization(X, Y, n_x=len(chars), char_indices=char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "with io.open('namesDataset.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40 \n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = open('MovieCorpus.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = open('TwitterConvCorpus.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"smt. rajni devismt rajni devisheedevismt. rajni devismt rajni devisheetalsangeetasmt saritashivanisangeetasmt. manjudeepasantosh baisangeetasmt. manjudeepasanto devishikhaanjukantasangeetasmt. manjurenusunitasmt saritashivanishivanishivanishivanishivanishivanishivanishivanishivanishivanishivanishivanipriyankarekha devishivanishivanishivanishivanishivanishivanipriyankashalunehashakuntlashakshishanda devismt. sunitasmt sa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 144854\n",
      "total chars: 91\n",
      "nb sequences: 48272\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "with io.open('namesDataset.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40 \n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame()\n",
    "data['sent'] = sentences\n",
    "data['target'] = next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                   sent target\n",
       "0        name\\nshivani\\nisha\\nsmt shyani devi\\ndivya\\n      m\n",
       "1        e\\nshivani\\nisha\\nsmt shyani devi\\ndivya\\nman      s\n",
       "2        hivani\\nisha\\nsmt shyani devi\\ndivya\\nmansi\\n      m\n",
       "3        ani\\nisha\\nsmt shyani devi\\ndivya\\nmansi\\nmaz      i\n",
       "4        \\nisha\\nsmt shyani devi\\ndivya\\nmansi\\nmazida     \\n\n",
       "5        ha\\nsmt shyani devi\\ndivya\\nmansi\\nmazida\\npo      o\n",
       "6         smt shyani devi\\ndivya\\nmansi\\nmazida\\npooja     \\n\n",
       "7         shyani devi\\ndivya\\nmansi\\nmazida\\npooja\\nka      j\n",
       "8        yani devi\\ndivya\\nmansi\\nmazida\\npooja\\nkajal     \\n\n",
       "9       i devi\\ndivya\\nmansi\\nmazida\\npooja\\nkajal\\nme      e\n",
       "10      evi\\ndivya\\nmansi\\nmazida\\npooja\\nkajal\\nmeena     \\n\n",
       "11     \\ndivya\\nmansi\\nmazida\\npooja\\nkajal\\nmeena\\nso      n\n",
       "12      vya\\nmansi\\nmazida\\npooja\\nkajal\\nmeena\\nsonam     \\n\n",
       "13     \\nmansi\\nmazida\\npooja\\nkajal\\nmeena\\nsonam\\nbu      i\n",
       "14      nsi\\nmazida\\npooja\\nkajal\\nmeena\\nsonam\\nbuity     \\n\n",
       "15     \\nmazida\\npooja\\nkajal\\nmeena\\nsonam\\nbuity\\nhi      n\n",
       "16     zida\\npooja\\nkajal\\nmeena\\nsonam\\nbuity\\nhina\\n      s\n",
       "17     a\\npooja\\nkajal\\nmeena\\nsonam\\nbuity\\nhina\\nsha      k\n",
       "18      ooja\\nkajal\\nmeena\\nsonam\\nbuity\\nhina\\nshaksh      i\n",
       "19      a\\nkajal\\nmeena\\nsonam\\nbuity\\nhina\\nshakshi s      a\n",
       "20       ajal\\nmeena\\nsonam\\nbuity\\nhina\\nshakshi saga      r\n",
       "21      l\\nmeena\\nsonam\\nbuity\\nhina\\nshakshi sagar\\np      o\n",
       "22       eena\\nsonam\\nbuity\\nhina\\nshakshi sagar\\npooj      a\n",
       "23      a\\nsonam\\nbuity\\nhina\\nshakshi sagar\\npooja\\na      n\n",
       "24       onam\\nbuity\\nhina\\nshakshi sagar\\npooja\\nanit      a\n",
       "25      m\\nbuity\\nhina\\nshakshi sagar\\npooja\\nanita\\nn      e\n",
       "26       uity\\nhina\\nshakshi sagar\\npooja\\nanita\\nneet      u\n",
       "27      y\\nhina\\nshakshi sagar\\npooja\\nanita\\nneetu\\na      n\n",
       "28       ina\\nshakshi sagar\\npooja\\nanita\\nneetu\\nansh      u\n",
       "29       \\nshakshi sagar\\npooja\\nanita\\nneetu\\nanshu d      /\n",
       "...                                                ...    ...\n",
       "48242     ta\\nkm. bhavna,\\nकु0 भावना\\nanjum\\nmiss reen      a\n",
       "48243     km. bhavna,\\nकु0 भावना\\nanjum\\nmiss reena\\np      o\n",
       "48244      bhavna,\\nकु0 भावना\\nanjum\\nmiss reena\\npooj      a\n",
       "48245    avna,\\nकु0 भावना\\nanjum\\nmiss reena\\npooja\\nr      a\n",
       "48246    a,\\nकु0 भावना\\nanjum\\nmiss reena\\npooja\\nrakh      i\n",
       "48247    कु0 भावना\\nanjum\\nmiss reena\\npooja\\nrakhi\\nm      u\n",
       "48248     भावना\\nanjum\\nmiss reena\\npooja\\nrakhi\\nmusa      r\n",
       "48249    वना\\nanjum\\nmiss reena\\npooja\\nrakhi\\nmusarra      t\n",
       "48250   \\nanjum\\nmiss reena\\npooja\\nrakhi\\nmusarrat\\ns      a\n",
       "48251    jum\\nmiss reena\\npooja\\nrakhi\\nmusarrat\\nsaro      j\n",
       "48252    \\nmiss reena\\npooja\\nrakhi\\nmusarrat\\nsaroj d      e\n",
       "48253     ss reena\\npooja\\nrakhi\\nmusarrat\\nsaroj devi     \\n\n",
       "48254    reena\\npooja\\nrakhi\\nmusarrat\\nsaroj devi\\nna      i\n",
       "48255    na\\npooja\\nrakhi\\nmusarrat\\nsaroj devi\\nnaina       \n",
       "48256     pooja\\nrakhi\\nmusarrat\\nsaroj devi\\nnaina @       g\n",
       "48257     ja\\nrakhi\\nmusarrat\\nsaroj devi\\nnaina @ gee      t\n",
       "48258     rakhi\\nmusarrat\\nsaroj devi\\nnaina @ geeta\\n      m\n",
       "48259     hi\\nmusarrat\\nsaroj devi\\nnaina @ geeta\\nman      j\n",
       "48260      musarrat\\nsaroj devi\\nnaina @ geeta\\nmanju       d\n",
       "48261      arrat\\nsaroj devi\\nnaina @ geeta\\nmanju d/0       \n",
       "48262      at\\nsaroj devi\\nnaina @ geeta\\nmanju d/0 ba      b\n",
       "48263       saroj devi\\nnaina @ geeta\\nmanju d/0 baboo       \n",
       "48264       oj devi\\nnaina @ geeta\\nmanju d/0 baboo la      l\n",
       "48265       devi\\nnaina @ geeta\\nmanju d/0 baboo lal j      a\n",
       "48266       i\\nnaina @ geeta\\nmanju d/0 baboo lal jata      v\n",
       "48267       aina @ geeta\\nmanju d/0 baboo lal jatav\\ns      h\n",
       "48268       a @ geeta\\nmanju d/0 baboo lal jatav\\nshiv      a\n",
       "48269        geeta\\nmanju d/0 baboo lal jatav\\nshivani     \\n\n",
       "48270      eta\\nmanju d/0 baboo lal jatav\\nshivani\\nna      y\n",
       "48271      \\nmanju d/0 baboo lal jatav\\nshivani\\nnayna     \\n\n",
       "\n",
       "[48272 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_values = 78\n",
    "reshapor = Reshape((1, n_values))\n",
    "LSTM_cell = LSTM(n_a, return_state=True)\n",
    "densor = Dense(n_values, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def djmodel(Tx, n_a, n_values):\n",
    "    \n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    outputs=[]\n",
    "    \n",
    "    \n",
    "    for t in range(Tx):\n",
    "        x = Lambda(lambda z:z[:,t,:])(X)\n",
    "        x = reshapor(x)\n",
    "        \n",
    "        a, _, c = LSTM_cell(inputs=x, initial_state=[a, c])\n",
    "        out = densor(a)\n",
    "        \n",
    "        outputs.append(out)\n",
    "        \n",
    "    model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = djmodel(Tx = 30, n_a = 64, n_values=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 78)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 78)        0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "a0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  36608       reshape_1[0][0]                  \n",
      "                                                                 a0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 reshape_1[1][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 reshape_1[2][0]                  \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 reshape_1[3][0]                  \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 reshape_1[4][0]                  \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 reshape_1[5][0]                  \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 reshape_1[6][0]                  \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 reshape_1[7][0]                  \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 reshape_1[8][0]                  \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 reshape_1[9][0]                  \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 reshape_1[10][0]                 \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 reshape_1[11][0]                 \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 reshape_1[12][0]                 \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 reshape_1[13][0]                 \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 reshape_1[14][0]                 \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 reshape_1[15][0]                 \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 reshape_1[16][0]                 \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 reshape_1[17][0]                 \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 reshape_1[18][0]                 \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 reshape_1[19][0]                 \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "                                                                 reshape_1[20][0]                 \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[19][2]                    \n",
      "                                                                 reshape_1[21][0]                 \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[20][2]                    \n",
      "                                                                 reshape_1[22][0]                 \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[21][2]                    \n",
      "                                                                 reshape_1[23][0]                 \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[22][2]                    \n",
      "                                                                 reshape_1[24][0]                 \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[23][2]                    \n",
      "                                                                 reshape_1[25][0]                 \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[24][2]                    \n",
      "                                                                 reshape_1[26][0]                 \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[25][2]                    \n",
      "                                                                 reshape_1[27][0]                 \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[26][2]                    \n",
      "                                                                 reshape_1[28][0]                 \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[27][2]                    \n",
      "                                                                 reshape_1[29][0]                 \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[28][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 78)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 78)           5070        lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "                                                                 lstm_1[20][0]                    \n",
      "                                                                 lstm_1[21][0]                    \n",
      "                                                                 lstm_1[22][0]                    \n",
      "                                                                 lstm_1[23][0]                    \n",
      "                                                                 lstm_1[24][0]                    \n",
      "                                                                 lstm_1[25][0]                    \n",
      "                                                                 lstm_1[26][0]                    \n",
      "                                                                 lstm_1[27][0]                    \n",
      "                                                                 lstm_1[28][0]                    \n",
      "                                                                 lstm_1[29][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,678\n",
      "Trainable params: 41,678\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.random((60, 30, 78))\n",
    "Y = np.random.random((30, 60, 78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/100\n",
      "60/60 [==============================] - 24s 394ms/step - loss: 5133.4975 - dense_1_loss: 170.3503 - dense_1_acc: 0.0000e+00 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0500 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0500                          \n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5108.5322 - dense_1_loss: 169.6008 - dense_1_acc: 0.0000e+00 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0667 - dense_1_acc_28: 0.0333 - dense_1_acc_29: 0.0500                        \n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5103.5376 - dense_1_loss: 169.3916 - dense_1_acc: 0.0000e+00 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0333 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0500 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0500 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0333 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0333 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00    \n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5102.4783 - dense_1_loss: 169.3411 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0333 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0333 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0500 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0167 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0333 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0333 - dense_1_acc_28: 0.0333 - dense_1_acc_29: 0.0000e+00                        \n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5102.1920 - dense_1_loss: 169.3291 - dense_1_acc: 0.0000e+00 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0333 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0333 - dense_1_acc_28: 0.0333 - dense_1_acc_29: 0.0000e+00                            \n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.9391 - dense_1_loss: 169.3211 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0667 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0333 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                \n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.7862 - dense_1_loss: 169.3163 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0667 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0000e+00                            \n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.7121 - dense_1_loss: 169.3158 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0333 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0333 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                    \n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.6562 - dense_1_loss: 169.3158 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0333 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0333 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.6166 - dense_1_loss: 169.3163 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0333 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0333 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.5733 - dense_1_loss: 169.3159 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0333 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                                    \n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.5368 - dense_1_loss: 169.3148 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0667 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0000e+00                        \n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.5083 - dense_1_loss: 169.3137 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4826 - dense_1_loss: 169.3125 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0500 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.4620 - dense_1_loss: 169.3122 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4484 - dense_1_loss: 169.3107 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0000e+00 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0167 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0333 - dense_1_acc_21: 0.0333 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0000e+00 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0333 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4361 - dense_1_loss: 169.3105 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0167 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0333 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0333                        \n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4273 - dense_1_loss: 169.3099 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4197 - dense_1_loss: 169.3094 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4132 - dense_1_loss: 169.3084 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0000e+00 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.4057 - dense_1_loss: 169.3078 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.3993 - dense_1_loss: 169.3073 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0167 - dense_1_acc_14: 0.0333 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167        \n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3930 - dense_1_loss: 169.3066 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0000e+00 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167    \n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3878 - dense_1_loss: 169.3065 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3813 - dense_1_loss: 169.3062 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0500 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0000e+00 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0500 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167        \n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3767 - dense_1_loss: 169.3059 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0167 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0167 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                        \n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3702 - dense_1_loss: 169.3058 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0000e+00 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0167 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3646 - dense_1_loss: 169.3059 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3615 - dense_1_loss: 169.3058 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3549 - dense_1_loss: 169.3056 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3492 - dense_1_loss: 169.3054 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3436 - dense_1_loss: 169.3050 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0333 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3382 - dense_1_loss: 169.3048 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0167 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3318 - dense_1_loss: 169.3046 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3262 - dense_1_loss: 169.3047 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.3199 - dense_1_loss: 169.3043 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3146 - dense_1_loss: 169.3041 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333        \n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.3072 - dense_1_loss: 169.3039 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0000e+00 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2997 - dense_1_loss: 169.3035 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2917 - dense_1_loss: 169.3032 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                        \n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2844 - dense_1_loss: 169.3026 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                    \n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2764 - dense_1_loss: 169.3024 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0667 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                \n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2685 - dense_1_loss: 169.3020 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2599 - dense_1_loss: 169.3016 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2498 - dense_1_loss: 169.3012 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2402 - dense_1_loss: 169.3007 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167        \n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2304 - dense_1_loss: 169.3003 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167        \n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2193 - dense_1_loss: 169.2997 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0167 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2124 - dense_1_loss: 169.2994 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.2034 - dense_1_loss: 169.2992 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                    \n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1905 - dense_1_loss: 169.2985 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                \n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1790 - dense_1_loss: 169.2984 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                        \n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1676 - dense_1_loss: 169.2974 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.1558 - dense_1_loss: 169.2968 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1445 - dense_1_loss: 169.2958 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1313 - dense_1_loss: 169.2950 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1187 - dense_1_loss: 169.2941 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.1069 - dense_1_loss: 169.2931 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0941 - dense_1_loss: 169.2927 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0333 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                        \n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0798 - dense_1_loss: 169.2922 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0500 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0333 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0665 - dense_1_loss: 169.2916 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0561 - dense_1_loss: 169.2915 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0377 - dense_1_loss: 169.2906 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0000e+00 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5101.0245 - dense_1_loss: 169.2900 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5101.0093 - dense_1_loss: 169.2892 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0000e+00 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9930 - dense_1_loss: 169.2882 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0167 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9781 - dense_1_loss: 169.2874 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0167 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9602 - dense_1_loss: 169.2865 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0333 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9444 - dense_1_loss: 169.2857 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9271 - dense_1_loss: 169.2844 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0000e+00 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                \n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.9076 - dense_1_loss: 169.2834 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.8862 - dense_1_loss: 169.2825 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0500 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.8688 - dense_1_loss: 169.2816 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0667 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.8458 - dense_1_loss: 169.2803 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0333 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                    \n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.8236 - dense_1_loss: 169.2796 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.8017 - dense_1_loss: 169.2783 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                        \n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.7756 - dense_1_loss: 169.2771 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.7607 - dense_1_loss: 169.2765 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0333 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                            \n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.7293 - dense_1_loss: 169.2764 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.7084 - dense_1_loss: 169.2746 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167            \n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.6771 - dense_1_loss: 169.2730 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0000e+00 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                                                \n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.6586 - dense_1_loss: 169.2708 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0500 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333                \n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.6301 - dense_1_loss: 169.2704 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0333 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167        \n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.6100 - dense_1_loss: 169.2703 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                \n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.5831 - dense_1_loss: 169.2687 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0500 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0500 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0333            \n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.5558 - dense_1_loss: 169.2673 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0500 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.5393 - dense_1_loss: 169.2665 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0333 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00    \n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.5105 - dense_1_loss: 169.2650 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0167 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00            \n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.4823 - dense_1_loss: 169.2634 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                            \n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.4584 - dense_1_loss: 169.2627 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0333 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0167 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0000e+00 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0333 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0167 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0167 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0167                \n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.4334 - dense_1_loss: 169.2628 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0000e+00 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                            \n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.4064 - dense_1_loss: 169.2612 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0000e+00 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                    \n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.3832 - dense_1_loss: 169.2600 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00        \n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.3600 - dense_1_loss: 169.2590 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                        \n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.3367 - dense_1_loss: 169.2583 - dense_1_acc: 0.0167 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0000e+00 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0000e+00                            \n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.3129 - dense_1_loss: 169.2574 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0000e+00 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0000e+00 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0333 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0000e+00                \n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.2831 - dense_1_loss: 169.2558 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0000e+00 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0000e+00 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0000e+00            \n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.2601 - dense_1_loss: 169.2540 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0333 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0167 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0667 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0333 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                                \n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5100.2301 - dense_1_loss: 169.2522 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0333 - dense_1_acc_18: 0.0333 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0500 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0167 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0000e+00 - dense_1_acc_29: 0.0167                            \n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 0s 1ms/step - loss: 5100.2072 - dense_1_loss: 169.2520 - dense_1_acc: 0.0333 - dense_1_acc_1: 0.0167 - dense_1_acc_2: 0.0167 - dense_1_acc_3: 0.0167 - dense_1_acc_4: 0.0167 - dense_1_acc_5: 0.0167 - dense_1_acc_6: 0.0000e+00 - dense_1_acc_7: 0.0333 - dense_1_acc_8: 0.0000e+00 - dense_1_acc_9: 0.0167 - dense_1_acc_10: 0.0000e+00 - dense_1_acc_11: 0.0167 - dense_1_acc_12: 0.0167 - dense_1_acc_13: 0.0500 - dense_1_acc_14: 0.0167 - dense_1_acc_15: 0.0000e+00 - dense_1_acc_16: 0.0000e+00 - dense_1_acc_17: 0.0167 - dense_1_acc_18: 0.0167 - dense_1_acc_19: 0.0000e+00 - dense_1_acc_20: 0.0167 - dense_1_acc_21: 0.0167 - dense_1_acc_22: 0.0000e+00 - dense_1_acc_23: 0.0333 - dense_1_acc_24: 0.0333 - dense_1_acc_25: 0.0167 - dense_1_acc_26: 0.0333 - dense_1_acc_27: 0.0000e+00 - dense_1_acc_28: 0.0167 - dense_1_acc_29: 0.0000e+00                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1f707bb38>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_emoji.csv')\n",
    "test = pd.read_csv('tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>never talk to me again</th>\n",
       "      <th>3</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love you mum</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            never talk to me again  3  Unnamed: 2 Unnamed: 3\n",
       "0  I am proud of your achievements  2         NaN        NaN\n",
       "1   It is the worst day in my life  3         NaN        NaN\n",
       "2                 Miss you so much  0         NaN        [0]\n",
       "3                     food is life  4         NaN        NaN\n",
       "4                   I love you mum  0         NaN        NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,0]\n",
    "Y_test = test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    3\n",
       "2    0\n",
       "3    4\n",
       "4    0\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = lb.fit_transform(Y_train)\n",
    "Y_oh_test = lb.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_oh_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(path):\n",
    "\n",
    "    word_to_vec_map = {}\n",
    "    words = set()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            currline = line.strip().split()\n",
    "            currWord = currline[0]\n",
    "            words.add(currWord)\n",
    "            currFeature = currline[1:]\n",
    "            word_to_vec_map[currWord]= np.array(currFeature, dtype=np.float64)\n",
    "\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        i=1\n",
    "        for word in sorted(words):\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            i+=1\n",
    "\n",
    "\n",
    "        return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove('/home/aniket/bruce/DeepLearning/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_average(sentence, word_to_vec_map):\n",
    "    words = sentence.lower().split()\n",
    "    shape = word_to_vec_map[words[0]].shape\n",
    "    avg = np.zeros(shape)\n",
    "    \n",
    "    total = 0\n",
    "    for w in words:\n",
    "        total +=word_to_vec_map[w]\n",
    "        \n",
    "    avg = total/len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_average(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence = X[i].lower().split()\n",
    "        for j,k in enumerate(sentence):\n",
    "            X_indices[i,j] = word_to_index[k]\n",
    "            \n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentence_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1\n",
    "    # Initialize the embedding matrix as a numpy array of zeros.\n",
    "    # See instructions above to choose the correct shape.\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Step 2\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Step 3\n",
    "    # Define Keras embedding layer with the correct input and output sizes\n",
    "    # Make it non-trainable.\n",
    "    embedding_layer = Embedding(vocab_len,emb_dim)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Step 4 (already done for you; please do not modify)\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojify(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(shape=(maxLen,), dtype=\"int32\")\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = Dense(5)(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aniket/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 20,223,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = emojify((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentence_to_indices(X_train, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "131/131 [==============================] - 9s 67ms/step - loss: 1.6179 - acc: 0.2672\n",
      "Epoch 2/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 1.5260 - acc: 0.2824\n",
      "Epoch 3/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 1.4896 - acc: 0.2748\n",
      "Epoch 4/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 1.4000 - acc: 0.4198\n",
      "Epoch 5/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 1.3001 - acc: 0.5267\n",
      "Epoch 6/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1923 - acc: 0.4885\n",
      "Epoch 7/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 1.0887 - acc: 0.5954\n",
      "Epoch 8/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.8970 - acc: 0.7099\n",
      "Epoch 9/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.8167 - acc: 0.7252\n",
      "Epoch 10/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.7031 - acc: 0.7252\n",
      "Epoch 11/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.5658 - acc: 0.8321\n",
      "Epoch 12/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.4991 - acc: 0.8168\n",
      "Epoch 13/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.5177 - acc: 0.8168\n",
      "Epoch 14/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.4222 - acc: 0.8626\n",
      "Epoch 15/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.4107 - acc: 0.8473\n",
      "Epoch 16/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.2986 - acc: 0.9160\n",
      "Epoch 17/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.3536 - acc: 0.8779\n",
      "Epoch 18/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.2245 - acc: 0.9466\n",
      "Epoch 19/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.2594 - acc: 0.9008\n",
      "Epoch 20/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.1563 - acc: 0.9542\n",
      "Epoch 21/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.1803 - acc: 0.9542\n",
      "Epoch 22/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0758 - acc: 0.9924\n",
      "Epoch 23/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.1089 - acc: 0.9695\n",
      "Epoch 24/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0646 - acc: 0.9847\n",
      "Epoch 25/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0727 - acc: 0.9847\n",
      "Epoch 26/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0524 - acc: 0.9847\n",
      "Epoch 27/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0472 - acc: 0.9924\n",
      "Epoch 28/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0152 - acc: 0.9924\n",
      "Epoch 33/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0203 - acc: 0.9924\n",
      "Epoch 34/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0476 - acc: 0.9847\n",
      "Epoch 35/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.1560 - acc: 0.9466\n",
      "Epoch 36/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.4105 - acc: 0.9084\n",
      "Epoch 37/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.2419 - acc: 0.9160\n",
      "Epoch 38/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.1092 - acc: 0.9695\n",
      "Epoch 39/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.1359 - acc: 0.9695\n",
      "Epoch 40/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.1700 - acc: 0.9542\n",
      "Epoch 41/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.1248 - acc: 0.9618\n",
      "Epoch 42/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0361 - acc: 0.9924\n",
      "Epoch 43/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0641 - acc: 0.9771\n",
      "Epoch 44/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0265 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0906 - acc: 0.9695\n",
      "Epoch 46/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "131/131 [==============================] - 1s 11ms/step - loss: 0.0083 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb19d7b2518>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_oh_train, epochs = 50, batch_size= 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 2s 41ms/step\n",
      "\n",
      "Test accuracy =  0.8545454469594088\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentence_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_oh_test)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = pd.read_csv('emojis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:😞 prediction: work is hard\t😄\n",
      "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
      "Expected emoji:😄 prediction: you brighten my day\t❤️\n",
      "Expected emoji:😞 prediction: she is a bully\t❤️\n",
      "Expected emoji:🍴 prediction: See you at the restaurant\t😄\n",
      "Expected emoji:😄 prediction: What you did was awesome\t😞\n",
      "Expected emoji:😞 prediction: go away\t⚾\n",
      "Expected emoji:🍴 prediction: I did not have breakfast 😄\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ emojis.loc[Y_test[i],'emoji'] + ' prediction: '+ X_test[i] + emojis.loc[num,'emoji'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😄'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis.loc[Y_test[3],'emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =np.array(['hey I love you'])\n",
    "sentence = sentence_to_indices(sentence, word_to_index, max_len = maxLen)\n",
    "pred = model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'❤️'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis.loc[np.argmax(pred),'emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
